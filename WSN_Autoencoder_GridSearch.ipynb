{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "DnClmil5ahZm",
        "outputId": "d845005e-d7df-485d-e610-df0419e7b6c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LIGHTWEIGHT INTELLIGENT INTRUSION DETECTION SYSTEM (LIIDS)\n",
            "FOR WIRELESS SENSOR NETWORKS USING DEEP AUTOENCODERS\n",
            "WITH GRID SEARCH HYPERPARAMETER TUNING\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "NSL-KDD DATASET EXPERIMENT WITH GRID SEARCH\n",
            "================================================================================\n",
            "\n",
            "Loading NSL-KDD dataset...\n",
            "Downloading NSL-KDD training set...\n",
            "Downloading NSL-KDD test set...\n",
            "NSL-KDD Training set shape: (125973, 43)\n",
            "NSL-KDD Test set shape: (22544, 43)\n",
            "Features shape after preprocessing: (148517, 122)\n",
            "Number of normal samples: 77054\n",
            "Number of attack samples: 71463\n",
            "Class distribution in training set: [67343 58630]\n",
            "Class distribution in test set: [ 9711 12833]\n",
            "Grid searching 243 hyperparameter combinations...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b7c73ea960fb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-b7c73ea960fb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[0;31m# Run NSL-KDD experiment with grid search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m     \u001b[0mnsl_kdd_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_nsl_kdd_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_grid_search\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0;31m# Run UNSW-NB15 experiment with grid search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b7c73ea960fb>\u001b[0m in \u001b[0;36mrun_nsl_kdd_experiment\u001b[0;34m(use_grid_search)\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;31m# Perform grid search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchLIIDS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m         \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;31m# Plot grid search results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b7c73ea960fb>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_test, y_test, n_jobs)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;31m# Perform grid search in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         results = Parallel(n_jobs=n_jobs)(\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_combination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_combinations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1701\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1703\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_abort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1704\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_abort\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;31m# scheduling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m             \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m             \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1615\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \"\"\"Shutdown the workers and restart a new one with the same parameters\n\u001b[1;32m    619\u001b[0m         \"\"\"\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/executor.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self, kill_workers)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# When workers are killed in a brutal manner, they cannot execute the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0;31m# is shutting down.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_global_shutdown_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m                 \u001b[0mexecutor_manager_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m                 \u001b[0m_threads_wakeups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor_manager_thread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Lightweight Intelligent Intrusion Detection System (LIIDS) for Wireless Sensor Networks\n",
        "Using Deep Autoencoders with Grid Search Hyperparameter Tuning\n",
        "\n",
        "Implementation of MSc thesis framework by Salisu Gaya, ZUBAIRU\n",
        "Department of Electronics and Telecommunications Engineering\n",
        "Ahmadu Bello University, Zaria\n",
        "\n",
        "This implementation includes:\n",
        "1. Data preprocessing for NSL-KDD and UNSW-NB15 datasets\n",
        "2. Deep autoencoder model architecture with Grid Search hyperparameter optimization\n",
        "3. Training with resource optimization (early stopping, bottleneck compression)\n",
        "4. Evaluation metrics (accuracy, recall, false alarm rate)\n",
        "5. Energy and bandwidth efficiency analysis\n",
        "\"\"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, save_model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
        "import time\n",
        "import os\n",
        "import requests\n",
        "from itertools import product\n",
        "import multiprocessing\n",
        "from joblib import Parallel, delayed\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
        "\n",
        "# Define color scheme for plots\n",
        "COLORS = {\n",
        "    'primary': '#003366',\n",
        "    'secondary': '#0066cc',\n",
        "    'accent': '#ff9900',\n",
        "    'light_bg': '#f5f5f5',\n",
        "    'text': '#333333'\n",
        "}\n",
        "\n",
        "class LIIDS:\n",
        "    \"\"\"\n",
        "    Lightweight Intelligent Intrusion Detection System (LIIDS) for WSNs\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=41, hidden_layers=[128, 64, 32],\n",
        "                 activation='relu', output_activation='sigmoid',\n",
        "                 dropout_rate=0.2, learning_rate=0.01,\n",
        "                 batch_size=32, epochs=50, l1_reg=0.01):\n",
        "        \"\"\"\n",
        "        Initialize the LIIDS model with hyperparameters\n",
        "\n",
        "        Args:\n",
        "            input_dim: Number of input features\n",
        "            hidden_layers: List of neurons in each hidden layer\n",
        "            activation: Activation function for hidden layers\n",
        "            output_activation: Activation function for output layer\n",
        "            dropout_rate: Dropout rate for regularization\n",
        "            learning_rate: Learning rate for the optimizer\n",
        "            batch_size: Batch size for training\n",
        "            epochs: Maximum number of epochs for training\n",
        "            l1_reg: L1 regularization coefficient\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.activation = activation\n",
        "        self.output_activation = output_activation\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.l1_reg = l1_reg\n",
        "        self.threshold = None\n",
        "        self.original_data_size = input_dim\n",
        "        self.compressed_data_size = hidden_layers[-1]  # Bottleneck size\n",
        "        self.model = None\n",
        "        self.encoder = None\n",
        "        self.history = None\n",
        "        self.training_time = 0\n",
        "        self.scaler = MinMaxScaler()\n",
        "\n",
        "        # Build the model\n",
        "        self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"\n",
        "        Build the deep autoencoder architecture\n",
        "        - Encoder: Input → Hidden Layers\n",
        "        - Decoder: Hidden Layers (reversed) → Output\n",
        "        - Activations: ReLU for hidden layers, Sigmoid for output\n",
        "        - Regularization: L1 and Dropout\n",
        "        \"\"\"\n",
        "        # Input layer\n",
        "        input_layer = Input(shape=(self.input_dim,), name='input')\n",
        "\n",
        "        # Encoder layers\n",
        "        encoded = input_layer\n",
        "        for i, units in enumerate(self.hidden_layers):\n",
        "            encoded = Dense(units=units,\n",
        "                           activation=self.activation,\n",
        "                           kernel_regularizer=l1(self.l1_reg),\n",
        "                           name=f'encoder_{i+1}')(encoded)\n",
        "            if i < len(self.hidden_layers) - 1:  # No dropout at bottleneck\n",
        "                encoded = Dropout(self.dropout_rate)(encoded)\n",
        "\n",
        "        # Bottleneck layer (last hidden layer)\n",
        "        bottleneck = encoded\n",
        "\n",
        "        # Decoder layers (reverse of encoder)\n",
        "        decoded = bottleneck\n",
        "        for i, units in enumerate(reversed(self.hidden_layers[:-1])):\n",
        "            decoded = Dense(units=units,\n",
        "                           activation=self.activation,\n",
        "                           kernel_regularizer=l1(self.l1_reg),\n",
        "                           name=f'decoder_{i+1}')(decoded)\n",
        "            decoded = Dropout(self.dropout_rate)(decoded)\n",
        "\n",
        "        # Output layer\n",
        "        output_layer = Dense(units=self.input_dim,\n",
        "                            activation=self.output_activation,\n",
        "                            name='output')(decoded)\n",
        "\n",
        "        # Create the autoencoder model\n",
        "        self.model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        # Create the encoder model (for bottleneck extraction)\n",
        "        self.encoder = Model(inputs=input_layer, outputs=bottleneck)\n",
        "\n",
        "        # Compile the model\n",
        "        self.model.compile(optimizer=Adam(learning_rate=self.learning_rate),\n",
        "                         loss='mse')\n",
        "\n",
        "    def preprocess_data(self, X, y=None, fit=True):\n",
        "        \"\"\"\n",
        "        Preprocess the data (normalization/scaling)\n",
        "        \"\"\"\n",
        "        # Scale features to [0,1] range\n",
        "        if fit:\n",
        "            X_scaled = self.scaler.fit_transform(X)\n",
        "        else:\n",
        "            X_scaled = self.scaler.transform(X)\n",
        "\n",
        "        return X_scaled, y\n",
        "\n",
        "    def train(self, X_train, validation_split=0.2, patience=10):\n",
        "        \"\"\"\n",
        "        Train the autoencoder model on normal data only\n",
        "        \"\"\"\n",
        "        print(f\"Training model with {self.hidden_layers} neurons, lr={self.learning_rate}, dropout={self.dropout_rate}, L1={self.l1_reg}, batch_size={self.batch_size}\")\n",
        "\n",
        "        # Define callbacks for early stopping\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=patience,\n",
        "            verbose=0,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        # Start timing\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Train the model\n",
        "        self.history = self.model.fit(\n",
        "            X_train, X_train,  # Autoencoder learns to reconstruct input\n",
        "            epochs=self.epochs,\n",
        "            batch_size=self.batch_size,\n",
        "            validation_split=validation_split,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=0  # Reduced verbosity for grid search\n",
        "        )\n",
        "\n",
        "        # Calculate training time\n",
        "        self.training_time = time.time() - start_time\n",
        "        print(f\"Training completed in {self.training_time:.2f} seconds\")\n",
        "\n",
        "        return self.history\n",
        "\n",
        "    def set_threshold(self, X_normal, percentile=95):\n",
        "        \"\"\"\n",
        "        Set the threshold for anomaly detection based on reconstruction error of normal data\n",
        "        \"\"\"\n",
        "        # Get reconstruction errors on normal data\n",
        "        predictions = self.model.predict(X_normal, verbose=0)\n",
        "        mse = np.mean(np.power(X_normal - predictions, 2), axis=1)\n",
        "\n",
        "        # Set threshold as the 95th percentile of errors\n",
        "        self.threshold = np.percentile(mse, percentile)\n",
        "        print(f\"Anomaly threshold set at {self.threshold:.6f} ({percentile}th percentile)\")\n",
        "\n",
        "        return self.threshold\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict anomalies based on reconstruction error\n",
        "        \"\"\"\n",
        "        if self.threshold is None:\n",
        "            raise ValueError(\"Threshold not set. Run set_threshold() first.\")\n",
        "\n",
        "        # Calculate reconstruction error\n",
        "        predictions = self.model.predict(X, verbose=0)\n",
        "        mse = np.mean(np.power(X - predictions, 2), axis=1)\n",
        "\n",
        "        # Classify based on threshold\n",
        "        y_pred = (mse > self.threshold).astype(int)\n",
        "\n",
        "        return y_pred, mse\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        \"\"\"\n",
        "        Evaluate the model performance\n",
        "        \"\"\"\n",
        "        # Get predictions\n",
        "        y_pred, mse = self.predict(X)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "        # Calculate confusion matrix\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "\n",
        "        # Calculate false alarm rate\n",
        "        far = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "\n",
        "        # Calculate ROC curve and AUC\n",
        "        fpr, tpr, _ = roc_curve(y_true, mse)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Return metrics\n",
        "        metrics = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'far': far,\n",
        "            'auc': roc_auc,\n",
        "            'confusion_matrix': {\n",
        "                'tn': tn,\n",
        "                'fp': fp,\n",
        "                'fn': fn,\n",
        "                'tp': tp\n",
        "            },\n",
        "            'roc': {\n",
        "                'fpr': fpr,\n",
        "                'tpr': tpr\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return metrics, mse\n",
        "\n",
        "    def calculate_resource_efficiency(self):\n",
        "        \"\"\"\n",
        "        Calculate bandwidth and energy efficiency\n",
        "        \"\"\"\n",
        "        # Calculate bandwidth efficiency (Eq. 3.9 in thesis)\n",
        "        bandwidth_efficiency = ((self.original_data_size - self.compressed_data_size) /\n",
        "                              self.original_data_size) * 100\n",
        "\n",
        "        # Calculate energy efficiency (simplified model)\n",
        "        energy_efficiency = bandwidth_efficiency\n",
        "\n",
        "        # Return efficiency metrics\n",
        "        efficiency = {\n",
        "            'original_size': self.original_data_size,\n",
        "            'compressed_size': self.compressed_data_size,\n",
        "            'bandwidth_efficiency': bandwidth_efficiency,\n",
        "            'energy_efficiency': energy_efficiency,\n",
        "            'data_reduction_ratio': self.original_data_size/self.compressed_data_size,\n",
        "            'training_time': self.training_time\n",
        "        }\n",
        "\n",
        "        return efficiency\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Plot training and validation loss over epochs\"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(self.history.history['loss'], label='Training Loss', color=COLORS['primary'])\n",
        "        plt.plot(self.history.history['val_loss'], label='Validation Loss', color=COLORS['accent'])\n",
        "        plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Epochs', fontsize=12)\n",
        "        plt.ylabel('Loss (MSE)', fontsize=12)\n",
        "        plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_confusion_matrix(self, metrics):\n",
        "        \"\"\"Plot confusion matrix\"\"\"\n",
        "        cm = np.array([\n",
        "            [metrics['confusion_matrix']['tn'], metrics['confusion_matrix']['fp']],\n",
        "            [metrics['confusion_matrix']['fn'], metrics['confusion_matrix']['tp']]\n",
        "        ])\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                   xticklabels=['Normal', 'Anomaly'],\n",
        "                   yticklabels=['Normal', 'Anomaly'])\n",
        "        plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "        plt.ylabel('True Label', fontsize=12)\n",
        "        plt.xlabel('Predicted Label', fontsize=12)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def save_model(self, path=\"liids_model.h5\"):\n",
        "        \"\"\"Save the trained model\"\"\"\n",
        "        self.model.save(path)\n",
        "        print(f\"Model saved to {path}\")\n",
        "\n",
        "\n",
        "class GridSearchLIIDS:\n",
        "    \"\"\"\n",
        "    Grid Search for hyperparameter tuning of LIIDS model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, param_grid=None, n_folds=5):\n",
        "        \"\"\"\n",
        "        Initialize the grid search with parameter grid\n",
        "\n",
        "        Args:\n",
        "            input_dim: Number of input features\n",
        "            param_grid: Dictionary of hyperparameters to tune\n",
        "            n_folds: Number of cross-validation folds\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.n_folds = n_folds\n",
        "        self.best_model = None\n",
        "        self.best_params = None\n",
        "        self.best_metrics = None\n",
        "        self.results = []\n",
        "\n",
        "        # Default parameter grid if none provided\n",
        "        if param_grid is None:\n",
        "            self.param_grid = {\n",
        "                'hidden_layers': [(128, 64, 32), (256, 128, 64), (64, 32, 16)],\n",
        "                'learning_rate': [0.001, 0.005, 0.01],\n",
        "                'batch_size': [32, 64, 128],\n",
        "                'dropout_rate': [0.1, 0.2, 0.3],\n",
        "                'l1_reg': [0.001, 0.005, 0.01]\n",
        "            }\n",
        "        else:\n",
        "            self.param_grid = param_grid\n",
        "\n",
        "    def _evaluate_model(self, X_train, y_train, X_test, y_test, params):\n",
        "        \"\"\"\n",
        "        Evaluate a single model with given parameters\n",
        "        \"\"\"\n",
        "        # Create and train model\n",
        "        model = LIIDS(\n",
        "            input_dim=self.input_dim,\n",
        "            hidden_layers=list(params['hidden_layers']),\n",
        "            dropout_rate=params['dropout_rate'],\n",
        "            learning_rate=params['learning_rate'],\n",
        "            batch_size=params['batch_size'],\n",
        "            l1_reg=params['l1_reg']\n",
        "        )\n",
        "\n",
        "        # Extract normal samples for training\n",
        "        X_train_normal = X_train[y_train == 0]\n",
        "\n",
        "        # Preprocess data\n",
        "        X_train_normal_scaled, _ = model.preprocess_data(X_train_normal)\n",
        "        X_test_scaled, y_test_scaled = model.preprocess_data(X_test, y_test, fit=False)\n",
        "\n",
        "        # Train model\n",
        "        model.train(X_train_normal_scaled, validation_split=0.2, patience=10)\n",
        "\n",
        "        # Set threshold based on normal data\n",
        "        model.set_threshold(X_train_normal_scaled)\n",
        "\n",
        "        # Evaluate model\n",
        "        metrics, _ = model.evaluate(X_test_scaled, y_test_scaled)\n",
        "\n",
        "        # Add params to metrics\n",
        "        metrics.update(params)\n",
        "\n",
        "        return metrics, model\n",
        "\n",
        "    def fit(self, X_train, y_train, X_test, y_test, n_jobs=None):\n",
        "        \"\"\"\n",
        "        Perform grid search to find the best hyperparameters\n",
        "\n",
        "        Args:\n",
        "            X_train: Training data\n",
        "            y_train: Training labels\n",
        "            X_test: Test data\n",
        "            y_test: Test labels\n",
        "            n_jobs: Number of parallel jobs (None = use all cores)\n",
        "        \"\"\"\n",
        "        # Generate all parameter combinations\n",
        "        param_combinations = list(dict(zip(self.param_grid.keys(), values))\n",
        "                               for values in product(*self.param_grid.values()))\n",
        "\n",
        "        print(f\"Grid searching {len(param_combinations)} hyperparameter combinations...\")\n",
        "\n",
        "        # Function to evaluate one parameter combination\n",
        "        def evaluate_combination(params):\n",
        "            try:\n",
        "                metrics, model = self._evaluate_model(X_train, y_train, X_test, y_test, params)\n",
        "                return metrics, model, params\n",
        "            except Exception as e:\n",
        "                print(f\"Error with params {params}: {e}\")\n",
        "                return None, None, params\n",
        "\n",
        "        # Set number of parallel jobs\n",
        "        if n_jobs is None:\n",
        "            n_jobs = max(1, multiprocessing.cpu_count() - 1)\n",
        "\n",
        "        # Perform grid search in parallel\n",
        "        results = Parallel(n_jobs=n_jobs)(\n",
        "            delayed(evaluate_combination)(params) for params in param_combinations\n",
        "        )\n",
        "\n",
        "        # Filter out None results (errors)\n",
        "        valid_results = [r for r in results if r[0] is not None]\n",
        "\n",
        "        # Sort by composite score (accuracy + recall - far)\n",
        "        sorted_results = sorted(\n",
        "            valid_results,\n",
        "            key=lambda x: (x[0]['accuracy'] + x[0]['recall'] - x[0]['far']),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        if not sorted_results:\n",
        "            raise ValueError(\"No valid parameter combinations found\")\n",
        "\n",
        "        # Get best results\n",
        "        best_metrics, best_model, best_params = sorted_results[0]\n",
        "\n",
        "        self.best_metrics = best_metrics\n",
        "        self.best_model = best_model\n",
        "        self.best_params = best_params\n",
        "        self.results = sorted_results\n",
        "\n",
        "        print(\"\\nGrid Search complete!\")\n",
        "        print(f\"Best parameters: {best_params}\")\n",
        "        print(f\"Best performance: Accuracy={best_metrics['accuracy']:.4f}, \"\n",
        "              f\"Recall={best_metrics['recall']:.4f}, \"\n",
        "              f\"FAR={best_metrics['far']:.4f}\")\n",
        "\n",
        "        return self.best_model, self.best_params, self.best_metrics\n",
        "\n",
        "    def plot_grid_search_results(self):\n",
        "        \"\"\"Plot grid search results\"\"\"\n",
        "        # Extract results for plotting\n",
        "        results_df = pd.DataFrame([r[0] for r in self.results])\n",
        "\n",
        "        # Plot accuracy vs. different hyperparameters\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "        # Learning rate vs accuracy\n",
        "        sns.boxplot(x='learning_rate', y='accuracy', data=results_df, ax=axes[0, 0])\n",
        "        axes[0, 0].set_title('Learning Rate vs Accuracy')\n",
        "        axes[0, 0].grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Dropout rate vs accuracy\n",
        "        sns.boxplot(x='dropout_rate', y='accuracy', data=results_df, ax=axes[0, 1])\n",
        "        axes[0, 1].set_title('Dropout Rate vs Accuracy')\n",
        "        axes[0, 1].grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "        # L1 regularization vs accuracy\n",
        "        sns.boxplot(x='l1_reg', y='accuracy', data=results_df, ax=axes[1, 0])\n",
        "        axes[1, 0].set_title('L1 Regularization vs Accuracy')\n",
        "        axes[1, 0].grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Batch size vs accuracy\n",
        "        sns.boxplot(x='batch_size', y='accuracy', data=results_df, ax=axes[1, 1])\n",
        "        axes[1, 1].set_title('Batch Size vs Accuracy')\n",
        "        axes[1, 1].grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot metrics for different network architectures\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        architecture_groups = results_df.groupby('hidden_layers').agg({\n",
        "            'accuracy': 'mean',\n",
        "            'recall': 'mean',\n",
        "            'precision': 'mean',\n",
        "            'far': 'mean'\n",
        "        }).reset_index()\n",
        "\n",
        "        architectures = [str(tuple(eval(arch))) for arch in architecture_groups['hidden_layers']]\n",
        "\n",
        "        x = np.arange(len(architectures))\n",
        "        width = 0.2\n",
        "\n",
        "        plt.bar(x - width*1.5, architecture_groups['accuracy'], width, label='Accuracy', color=COLORS['primary'])\n",
        "        plt.bar(x - width/2, architecture_groups['recall'], width, label='Recall', color=COLORS['secondary'])\n",
        "        plt.bar(x + width/2, architecture_groups['precision'], width, label='Precision', color=COLORS['accent'])\n",
        "        plt.bar(x + width*1.5, architecture_groups['far'], width, label='FAR', color='red')\n",
        "\n",
        "        plt.xlabel('Network Architecture (hidden layers)')\n",
        "        plt.ylabel('Metric Value')\n",
        "        plt.title('Performance Metrics by Network Architecture')\n",
        "        plt.xticks(x, architectures)\n",
        "        plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"\n",
        "    Class for handling data loading and preprocessing\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_nsl_kdd(train_path=None, test_path=None):\n",
        "        \"\"\"\n",
        "        Load and preprocess the NSL-KDD dataset\n",
        "        \"\"\"\n",
        "        print(\"\\nLoading NSL-KDD dataset...\")\n",
        "\n",
        "        # Define column names\n",
        "        columns = [\n",
        "            'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
        "            'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
        "            'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
        "            'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
        "            'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
        "            'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
        "            'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
        "            'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
        "            'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
        "            'dst_host_srv_rerror_rate', 'label', 'difficulty'\n",
        "        ]\n",
        "\n",
        "        # If paths are not provided, download the dataset\n",
        "        if train_path is None or test_path is None:\n",
        "            train_path = 'NSL-KDD/KDDTrain+.txt'\n",
        "            test_path = 'NSL-KDD/KDDTest+.txt'\n",
        "\n",
        "            # Create directory if it doesn't exist\n",
        "            if not os.path.exists('NSL-KDD'):\n",
        "                os.makedirs('NSL-KDD')\n",
        "\n",
        "            # Download files if they don't exist\n",
        "            if not os.path.exists(train_path):\n",
        "                print(\"Downloading NSL-KDD training set...\")\n",
        "                train_url = 'https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain%2B.txt'\n",
        "                response = requests.get(train_url)\n",
        "                with open(train_path, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "\n",
        "            if not os.path.exists(test_path):\n",
        "                print(\"Downloading NSL-KDD test set...\")\n",
        "                test_url = 'https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTest%2B.txt'\n",
        "                response = requests.get(test_url)\n",
        "                with open(test_path, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "\n",
        "        # Load data\n",
        "        train_data = pd.read_csv(train_path, header=None, names=columns)\n",
        "        test_data = pd.read_csv(test_path, header=None, names=columns)\n",
        "\n",
        "        # Display dataset information\n",
        "        print(f\"NSL-KDD Training set shape: {train_data.shape}\")\n",
        "        print(f\"NSL-KDD Test set shape: {test_data.shape}\")\n",
        "\n",
        "        # Combine train and test for preprocessing\n",
        "        data = pd.concat([train_data, test_data], axis=0)\n",
        "\n",
        "        # Convert labels to binary (normal vs attack)\n",
        "        data['binary_label'] = data['label'].apply(lambda x: 0 if x == 'normal' else 1)\n",
        "\n",
        "        # One-hot encode categorical features\n",
        "        categorical_cols = ['protocol_type', 'service', 'flag']\n",
        "        data = pd.get_dummies(data, columns=categorical_cols, drop_first=False)\n",
        "\n",
        "        # Drop unnecessary columns\n",
        "        data = data.drop(['label', 'difficulty'], axis=1)\n",
        "\n",
        "        # Extract features and labels\n",
        "        X = data.drop('binary_label', axis=1)\n",
        "        y = data['binary_label'].values\n",
        "\n",
        "        # Split back into train and test (maintain original split)\n",
        "        train_size = train_data.shape[0]\n",
        "        X_train = X.iloc[:train_size]\n",
        "        y_train = y[:train_size]\n",
        "        X_test = X.iloc[train_size:]\n",
        "        y_test = y[train_size:]\n",
        "\n",
        "        print(f\"Features shape after preprocessing: {X.shape}\")\n",
        "        print(f\"Number of normal samples: {(y == 0).sum()}\")\n",
        "        print(f\"Number of attack samples: {(y == 1).sum()}\")\n",
        "        print(f\"Class distribution in training set: {np.bincount(y_train)}\")\n",
        "        print(f\"Class distribution in test set: {np.bincount(y_test)}\")\n",
        "\n",
        "        return X_train, y_train, X_test, y_test\n",
        "\n",
        "    @staticmethod\n",
        "    def load_unsw_nb15(train_path=None, test_path=None):\n",
        "        \"\"\"\n",
        "        Load and preprocess the UNSW-NB15 dataset\n",
        "        \"\"\"\n",
        "        print(\"\\nLoading UNSW-NB15 dataset...\")\n",
        "\n",
        "        try:\n",
        "            # If paths are not provided, download the dataset\n",
        "            if train_path is None or test_path is None:\n",
        "                train_path = 'UNSW-NB15/UNSW-NB15_TRAIN.csv'\n",
        "                test_path = 'UNSW-NB15/UNSW-NB15_TEST.csv'\n",
        "\n",
        "                # Create directory if it doesn't exist\n",
        "                if not os.path.exists('UNSW-NB15'):\n",
        "                    os.makedirs('UNSW-NB15')\n",
        "\n",
        "                # Try multiple sources for downloading\n",
        "                sources = [\n",
        "                    ('https://research.unsw.edu.au/sites/default/files/documents/UNSW_NB15_TRAIN.csv',\n",
        "                     'https://research.unsw.edu.au/sites/default/files/documents/UNSW_NB15_TEST.csv'),\n",
        "                    ('https://raw.githubusercontent.com/defcom17/NSL_KDD/master/UNSW-NB15_TRAIN.csv',\n",
        "                     'https://raw.githubusercontent.com/defcom17/NSL_KDD/master/UNSW-NB15_TEST.csv')\n",
        "                ]\n",
        "\n",
        "                for train_url, test_url in sources:\n",
        "                    try:\n",
        "                        # Download training set\n",
        "                        if not os.path.exists(train_path):\n",
        "                            print(f\"Trying to download from {train_url}...\")\n",
        "                            response = requests.get(train_url, timeout=10)\n",
        "                            response.raise_for_status()\n",
        "                            with open(train_path, 'wb') as f:\n",
        "                                f.write(response.content)\n",
        "\n",
        "                        # Download test set\n",
        "                        if not os.path.exists(test_path):\n",
        "                            print(f\"Trying to download from {test_url}...\")\n",
        "                            response = requests.get(test_url, timeout=10)\n",
        "                            response.raise_for_status()\n",
        "                            with open(test_path, 'wb') as f:\n",
        "                                f.write(response.content)\n",
        "\n",
        "                        # If both files exist, break the loop\n",
        "                        if os.path.exists(train_path) and os.path.exists(test_path):\n",
        "                            break\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Download failed: {e}\")\n",
        "                        continue\n",
        "\n",
        "            # Load data\n",
        "            train_data = pd.read_csv(train_path, low_memory=False)\n",
        "            test_data = pd.read_csv(test_path, low_memory=False)\n",
        "\n",
        "            # Verify data format\n",
        "            if 'label' not in train_data.columns or 'label' not in test_data.columns:\n",
        "                raise ValueError(\"Dataset is missing 'label' column\")\n",
        "\n",
        "            # Display dataset information\n",
        "            print(f\"UNSW-NB15 Training set shape: {train_data.shape}\")\n",
        "            print(f\"UNSW-NB15 Test set shape: {test_data.shape}\")\n",
        "\n",
        "            # Combine train and test for preprocessing\n",
        "            data = pd.concat([train_data, test_data], axis=0)\n",
        "\n",
        "            # Drop irrelevant columns\n",
        "            if 'id' in data.columns:\n",
        "                data = data.drop(['id'], axis=1)\n",
        "            if 'attack_cat' in data.columns:\n",
        "                data = data.drop(['attack_cat'], axis=1)\n",
        "\n",
        "            # Handle missing values\n",
        "            numeric_cols = data.select_dtypes(include=['number']).columns\n",
        "            for col in numeric_cols:\n",
        "                if data[col].isnull().sum() > 0:\n",
        "                    data[col] = data[col].fillna(data[col].median())\n",
        "\n",
        "            categorical_cols = data.select_dtypes(include=['object']).columns\n",
        "            for col in categorical_cols:\n",
        "                if data[col].isnull().sum() > 0:\n",
        "                    data[col] = data[col].fillna(data[col].mode()[0])\n",
        "\n",
        "            # One-hot encode categorical features\n",
        "            categorical_cols = [col for col in ['proto', 'service', 'state'] if col in data.columns]\n",
        "            data = pd.get_dummies(data, columns=categorical_cols, drop_first=False)\n",
        "\n",
        "            # Extract features and labels\n",
        "            X = data.drop('label', axis=1)\n",
        "            y = data['label'].values\n",
        "\n",
        "            # Split back into train and test (maintain original split)\n",
        "            train_size = train_data.shape[0]\n",
        "            X_train = X.iloc[:train_size]\n",
        "            y_train = y[:train_size]\n",
        "            X_test = X.iloc[train_size:]\n",
        "            y_test = y[train_size:]\n",
        "\n",
        "            print(f\"Features shape after preprocessing: {X.shape}\")\n",
        "            print(f\"Number of normal samples: {(y == 0).sum()}\")\n",
        "            print(f\"Number of attack samples: {(y == 1).sum()}\")\n",
        "            print(f\"Class distribution in training set: {np.bincount(y_train)}\")\n",
        "            print(f\"Class distribution in test set: {np.bincount(y_test)}\")\n",
        "\n",
        "            return X_train, y_train, X_test, y_test\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading UNSW-NB15 dataset: {e}\")\n",
        "            print(\"Please download the UNSW-NB15 dataset manually and place in UNSW-NB15/ directory\")\n",
        "            return None, None, None, None\n",
        "\n",
        "\n",
        "def run_nsl_kdd_experiment(use_grid_search=True):\n",
        "    \"\"\"Run experiment on NSL-KDD dataset with grid search\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"NSL-KDD DATASET EXPERIMENT WITH GRID SEARCH\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Load NSL-KDD dataset\n",
        "    X_train, y_train, X_test, y_test = DataProcessor.load_nsl_kdd()\n",
        "\n",
        "    if use_grid_search:\n",
        "        # Define parameter grid\n",
        "        param_grid = {\n",
        "            'hidden_layers': [(128, 64, 32), (256, 128, 64), (64, 32, 16)],\n",
        "            'learning_rate': [0.001, 0.005, 0.01],\n",
        "            'batch_size': [32, 64, 128],\n",
        "            'dropout_rate': [0.1, 0.2, 0.3],\n",
        "            'l1_reg': [0.001, 0.005, 0.01]\n",
        "        }\n",
        "\n",
        "        # Perform grid search\n",
        "        grid_search = GridSearchLIIDS(input_dim=X_train.shape[1], param_grid=param_grid)\n",
        "        best_model, best_params, best_metrics = grid_search.fit(X_train, y_train, X_test, y_test)\n",
        "\n",
        "        # Plot grid search results\n",
        "        grid_search.plot_grid_search_results()\n",
        "\n",
        "        # Print best hyperparameters\n",
        "        print(\"\\nBest Hyperparameters:\")\n",
        "        for param, value in best_params.items():\n",
        "            print(f\"{param}: {value}\")\n",
        "\n",
        "        # Print best metrics\n",
        "        print(\"\\nBest Model Performance:\")\n",
        "        for metric in ['accuracy', 'precision', 'recall', 'f1', 'far', 'auc']:\n",
        "            print(f\"{metric}: {best_metrics[metric]:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        best_model.save_model(\"best_nsl_kdd_model.h5\")\n",
        "\n",
        "        return best_model, best_metrics, best_params\n",
        "\n",
        "    else:\n",
        "        # Use default hyperparameters without grid search\n",
        "        print(\"\\nRunning with default hyperparameters (no grid search)\")\n",
        "\n",
        "        # Extract normal samples for training the autoencoder\n",
        "        X_train_normal = X_train[y_train == 0]\n",
        "\n",
        "        # Initialize and train LIIDS\n",
        "        liids = LIIDS(input_dim=X_train.shape[1])\n",
        "\n",
        "        # Preprocess data\n",
        "        X_train_normal_scaled, _ = liids.preprocess_data(X_train_normal)\n",
        "        X_test_scaled, y_test_scaled = liids.preprocess_data(X_test, y_test, fit=False)\n",
        "\n",
        "        # Train the model\n",
        "        liids.train(X_train_normal_scaled, validation_split=0.2, patience=10)\n",
        "\n",
        "        # Set threshold based on normal data\n",
        "        liids.set_threshold(X_train_normal_scaled)\n",
        "\n",
        "        # Evaluate model\n",
        "        metrics, _ = liids.evaluate(X_test_scaled, y_test_scaled)\n",
        "\n",
        "        # Print metrics\n",
        "        print(\"\\nDefault Model Performance:\")\n",
        "        for metric in ['accuracy', 'precision', 'recall', 'f1', 'far', 'auc']:\n",
        "            print(f\"{metric}: {metrics[metric]:.4f}\")\n",
        "\n",
        "        # Save model\n",
        "        liids.save_model(\"default_nsl_kdd_model.h5\")\n",
        "\n",
        "        return liids, metrics, None\n",
        "\n",
        "\n",
        "def run_unsw_nb15_experiment(use_grid_search=True):\n",
        "    \"\"\"Run experiment on UNSW-NB15 dataset with grid search\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"UNSW-NB15 DATASET EXPERIMENT WITH GRID SEARCH\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Load UNSW-NB15 dataset\n",
        "    X_train, y_train, X_test, y_test = DataProcessor.load_unsw_nb15()\n",
        "\n",
        "    # Check if dataset loaded successfully\n",
        "    if X_train is None:\n",
        "        print(\"Skipping UNSW-NB15 experiment due to dataset loading failure\")\n",
        "        return None, None, None\n",
        "\n",
        "    if use_grid_search:\n",
        "        # Define parameter grid\n",
        "        param_grid = {\n",
        "            'hidden_layers': [(128, 64, 32), (256, 128, 64), (64, 32, 16)],\n",
        "            'learning_rate': [0.001, 0.005, 0.01],\n",
        "            'batch_size': [32, 64, 128],\n",
        "            'dropout_rate': [0.1, 0.2, 0.3],\n",
        "            'l1_reg': [0.001, 0.005, 0.01]\n",
        "        }\n",
        "\n",
        "        # Perform grid search\n",
        "        grid_search = GridSearchLIIDS(input_dim=X_train.shape[1], param_grid=param_grid)\n",
        "        best_model, best_params, best_metrics = grid_search.fit(X_train, y_train, X_test, y_test)\n",
        "\n",
        "        # Plot grid search results\n",
        "        grid_search.plot_grid_search_results()\n",
        "\n",
        "        # Print best hyperparameters\n",
        "        print(\"\\nBest Hyperparameters:\")\n",
        "        for param, value in best_params.items():\n",
        "            print(f\"{param}: {value}\")\n",
        "\n",
        "        # Print best metrics\n",
        "        print(\"\\nBest Model Performance:\")\n",
        "        for metric in ['accuracy', 'precision', 'recall', 'f1', 'far', 'auc']:\n",
        "            print(f\"{metric}: {best_metrics[metric]:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        best_model.save_model(\"best_unsw_nb15_model.h5\")\n",
        "\n",
        "        return best_model, best_metrics, best_params\n",
        "\n",
        "    else:\n",
        "        # Use default hyperparameters without grid search\n",
        "        print(\"\\nRunning with default hyperparameters (no grid search)\")\n",
        "\n",
        "        # Extract normal samples for training the autoencoder\n",
        "        X_train_normal = X_train[y_train == 0]\n",
        "\n",
        "        # Initialize and train LIIDS\n",
        "        liids = LIIDS(input_dim=X_train.shape[1])\n",
        "\n",
        "        # Preprocess data\n",
        "        X_train_normal_scaled, _ = liids.preprocess_data(X_train_normal)\n",
        "        X_test_scaled, y_test_scaled = liids.preprocess_data(X_test, y_test, fit=False)\n",
        "\n",
        "        # Train the model\n",
        "        liids.train(X_train_normal_scaled, validation_split=0.2, patience=10)\n",
        "\n",
        "        # Set threshold based on normal data\n",
        "        liids.set_threshold(X_train_normal_scaled)\n",
        "\n",
        "        # Evaluate model\n",
        "        metrics, _ = liids.evaluate(X_test_scaled, y_test_scaled)\n",
        "\n",
        "        # Print metrics\n",
        "        print(\"\\nDefault Model Performance:\")\n",
        "        for metric in ['accuracy', 'precision', 'recall', 'f1', 'far', 'auc']:\n",
        "            print(f\"{metric}: {metrics[metric]:.4f}\")\n",
        "\n",
        "        # Save model\n",
        "        liids.save_model(\"default_unsw_nb15_model.h5\")\n",
        "\n",
        "        return liids, metrics, None\n",
        "\n",
        "\n",
        "def compare_results(nsl_kdd_results, unsw_nb15_results):\n",
        "    \"\"\"Compare results from different datasets\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RESULTS COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Unpack results\n",
        "    nsl_kdd_model, nsl_kdd_metrics, nsl_kdd_params = nsl_kdd_results\n",
        "\n",
        "    if unsw_nb15_results[0] is None:\n",
        "        print(\"Cannot compare results because UNSW-NB15 experiment failed\")\n",
        "\n",
        "        # Create comparison table with just NSL-KDD\n",
        "        results = pd.DataFrame({\n",
        "            'NSL-KDD': [\n",
        "                nsl_kdd_metrics['accuracy'],\n",
        "                nsl_kdd_metrics['precision'],\n",
        "                nsl_kdd_metrics['recall'],\n",
        "                nsl_kdd_metrics['f1'],\n",
        "                nsl_kdd_metrics['far'],\n",
        "                nsl_kdd_metrics['auc']\n",
        "            ]\n",
        "        }, index=['Accuracy', 'Precision', 'Recall (DR)', 'F1-Score', 'False Alarm Rate', 'AUC'])\n",
        "\n",
        "        # Format percentages\n",
        "        results = results.applymap(lambda x: f\"{x:.4f}\")\n",
        "\n",
        "        print(\"\\nPerformance Metrics:\")\n",
        "        print(results)\n",
        "\n",
        "        return results\n",
        "\n",
        "    unsw_nb15_model, unsw_nb15_metrics, unsw_nb15_params = unsw_nb15_results\n",
        "\n",
        "    # Create comparison table\n",
        "    results = pd.DataFrame({\n",
        "        'NSL-KDD': [\n",
        "            nsl_kdd_metrics['accuracy'],\n",
        "            nsl_kdd_metrics['precision'],\n",
        "            nsl_kdd_metrics['recall'],\n",
        "            nsl_kdd_metrics['f1'],\n",
        "            nsl_kdd_metrics['far'],\n",
        "            nsl_kdd_metrics['auc']\n",
        "        ],\n",
        "        'UNSW-NB15': [\n",
        "            unsw_nb15_metrics['accuracy'],\n",
        "            unsw_nb15_metrics['precision'],\n",
        "            unsw_nb15_metrics['recall'],\n",
        "            unsw_nb15_metrics['f1'],\n",
        "            unsw_nb15_metrics['far'],\n",
        "            unsw_nb15_metrics['auc']\n",
        "        ]\n",
        "    }, index=['Accuracy', 'Precision', 'Recall (DR)', 'F1-Score', 'False Alarm Rate', 'AUC'])\n",
        "\n",
        "    # Format percentages\n",
        "    results = results.applymap(lambda x: f\"{x:.4f}\")\n",
        "\n",
        "    print(\"\\nPerformance Metrics Comparison:\")\n",
        "    print(results)\n",
        "\n",
        "    # Compare with results from thesis\n",
        "    thesis_results = pd.DataFrame({\n",
        "        'NSL-KDD (Thesis)': ['0.9976', '0.9935', '0.0065'],\n",
        "        'UNSW-NB15 (Thesis)': ['0.9852', '0.9873', '0.0127'],\n",
        "        'NSL-KDD (Grid Search)': [\n",
        "            f\"{nsl_kdd_metrics['accuracy']:.4f}\",\n",
        "            f\"{nsl_kdd_metrics['recall']:.4f}\",\n",
        "            f\"{nsl_kdd_metrics['far']:.4f}\"\n",
        "        ],\n",
        "        'UNSW-NB15 (Grid Search)': [\n",
        "            f\"{unsw_nb15_metrics['accuracy']:.4f}\",\n",
        "            f\"{unsw_nb15_metrics['recall']:.4f}\",\n",
        "            f\"{unsw_nb15_metrics['far']:.4f}\"\n",
        "        ]\n",
        "    }, index=['Accuracy', 'Recall (DR)', 'False Alarm Rate'])\n",
        "\n",
        "    print(\"\\nComparison with Thesis Results:\")\n",
        "    print(thesis_results)\n",
        "\n",
        "    # Plot comparison\n",
        "    metrics = ['Accuracy', 'Recall (DR)', 'False Alarm Rate']\n",
        "    datasets = ['NSL-KDD', 'UNSW-NB15']\n",
        "\n",
        "    # Create figure with multiple subplots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Get values\n",
        "        thesis_values = [float(thesis_results.loc[metric, f'{ds} (Thesis)']) for ds in datasets]\n",
        "        grid_search_values = [float(thesis_results.loc[metric, f'{ds} (Grid Search)'].replace(',', '.'))\n",
        "                              for ds in datasets]\n",
        "\n",
        "        # Create grouped bar chart\n",
        "        x = np.arange(len(datasets))\n",
        "        width = 0.35\n",
        "\n",
        "        ax.bar(x - width/2, thesis_values, width, label='Thesis Results', color=COLORS['primary'])\n",
        "        ax.bar(x + width/2, grid_search_values, width, label='Grid Search Results', color=COLORS['accent'])\n",
        "\n",
        "        # Add labels and title\n",
        "        ax.set_title(metric, fontsize=14, fontweight='bold')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(datasets)\n",
        "        ax.set_ylim(0, 1.0 if metric != 'False Alarm Rate' else 0.1)\n",
        "\n",
        "        # Add value labels\n",
        "        for j, v in enumerate(thesis_values):\n",
        "            ax.text(j - width/2, v + 0.01, f\"{v:.4f}\", ha='center', fontsize=10)\n",
        "\n",
        "        for j, v in enumerate(grid_search_values):\n",
        "            ax.text(j + width/2, v + 0.01, f\"{v:.4f}\", ha='center', fontsize=10)\n",
        "\n",
        "        # Add legend and grid\n",
        "        if i == 1:\n",
        "            ax.legend()\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Return results dataframe\n",
        "    return results\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run all experiments\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"LIGHTWEIGHT INTELLIGENT INTRUSION DETECTION SYSTEM (LIIDS)\")\n",
        "    print(\"FOR WIRELESS SENSOR NETWORKS USING DEEP AUTOENCODERS\")\n",
        "    print(\"WITH GRID SEARCH HYPERPARAMETER TUNING\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Run NSL-KDD experiment with grid search\n",
        "    nsl_kdd_results = run_nsl_kdd_experiment(use_grid_search=True)\n",
        "\n",
        "    # Run UNSW-NB15 experiment with grid search\n",
        "    unsw_nb15_results = run_unsw_nb15_experiment(use_grid_search=True)\n",
        "\n",
        "    # Compare results\n",
        "    compare_results(nsl_kdd_results, unsw_nb15_results)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"GRID SEARCH EXPERIMENTS COMPLETED SUCCESSFULLY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}